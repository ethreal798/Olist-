{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a5083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 16:06:41,458 [INFO] [EXTRACT] 正在加载数据: ..\\raw\\olist_orders_dataset.csv\n",
      "2025-10-15 16:06:41,715 [INFO] 加载完成，共99441行，8列\n",
      "2025-10-15 16:06:41,818 [INFO] [DATA QUALITY] 原始数据 - 行数: 99441 | 列数: 8 | 缺失率: 0.62% | 重复率: 0.00%\n",
      "2025-10-15 16:06:41,848 [INFO] 2016年9月的记录数: 4\n",
      "2025-10-15 16:06:41,853 [INFO] 2016年10月的记录数: 324\n",
      "2025-10-15 16:06:41,855 [INFO] 2016年12月的记录数: 1\n",
      "2025-10-15 16:06:41,855 [INFO] 2018年9月的记录数: 16\n",
      "2025-10-15 16:06:41,857 [INFO] 2018年10月的记录数: 4\n",
      "2025-10-15 16:06:41,884 [INFO] [TRANSFORM] 查看缺失值......\n",
      "2025-10-15 16:06:41,897 [INFO] 缺失值总数为4739\n",
      "2025-10-15 16:06:41,913 [INFO] 缺失列情况: \n",
      "order_approved_at                 135\n",
      "order_delivered_carrier_date     1716\n",
      "order_delivered_customer_date    2888\n",
      "dtype: int64\n",
      "2025-10-15 16:06:41,914 [INFO] order_approved_at 为时间列,缺失数:135进行时间填充\n",
      "2025-10-15 16:06:41,916 [INFO] 对order_approved_at列填充完成\n",
      "2025-10-15 16:06:41,919 [INFO] order_delivered_carrier_date 为时间列,缺失数:1716进行时间填充\n",
      "2025-10-15 16:06:41,923 [INFO] 对order_delivered_carrier_date列填充完成\n",
      "2025-10-15 16:06:41,924 [INFO] order_delivered_customer_date 为时间列,缺失数:2888进行时间填充\n",
      "2025-10-15 16:06:41,930 [INFO] 对order_delivered_customer_date列填充完成\n",
      "2025-10-15 16:06:41,931 [INFO] 缺失值填充完成\n",
      "2025-10-15 16:06:42,006 [INFO] 无重复值\n",
      "2025-10-15 16:06:42,113 [INFO] [DATA QUALITY] 清洗后数据 - 行数: 99092 | 列数: 11 | 缺失率: 0.00% | 重复率: 0.00%\n",
      "2025-10-15 16:06:42,602 [INFO] [LOAD] 数据已保存至..\\processed\\order_table_cleaned.csv\n",
      "2025-10-15 16:06:42,606 [INFO]  数据质量报告已保存至: ..\\processed\\data_quality_report.csv\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import process\n",
    "from nt import error\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"etl.log\", encoding='utf-8'), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "data_locate = Path('../raw/olist_orders_dataset.csv')\n",
    "save_cleaned = Path('../processed')\n",
    "save_cleaned.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def data_quality_report(df,stage_name):\n",
    "    \"\"\"\n",
    "    输出数据质量报告（重复率、缺失率等）\n",
    "    :param df: DataFrame\n",
    "    :param stage_name: 阶段名称，如 '原始数据' 或 '清洗后的数据'\n",
    "    :return: dict (用于后续保存或对比)\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    total_cols = df.shape[1]\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    duplicated_count = df.duplicated().sum()\n",
    "\n",
    "    # 计算比例\n",
    "    missing_rate = (missing_count / (total_rows * total_cols) * 100) if total_rows > 0 else 0\n",
    "    duplicated_rate = (duplicated_count / total_rows * 100) if total_rows > 0 else 0\n",
    "\n",
    "    # 打印日志\n",
    "    logging.info(f'[DATA QUALITY] {stage_name} - 行数: {total_rows} | 列数: {total_cols} | 缺失率: {missing_rate:.2f}% | 重复率: {duplicated_rate:.2f}%')\n",
    "\n",
    "    # 返回结构化结果，方便后面汇总\n",
    "    return {\n",
    "        'stage': stage_name,\n",
    "        'rows': total_rows,\n",
    "        'cols': total_cols,\n",
    "        'missing_count': missing_count,\n",
    "        'missing_rate': round(missing_rate, 2),\n",
    "        'duplicated_count': duplicated_count,\n",
    "        'duplicated_rate': round(duplicated_rate, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_data(path):\n",
    "    \"\"\"\n",
    "        从指定路径加载CSV文件\n",
    "    :param path: CSV文件路径\n",
    "    :return: 加载后的DataFrame\n",
    "    \"\"\"\n",
    "    logging.info(f'[EXTRACT] 正在加载数据: {path}')\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f'文件未找到: {path}')\n",
    "        return pd.DataFrame()\n",
    "    row,col = df.shape\n",
    "    logging.info(f'加载完成，共{row}行，{col}列')\n",
    "    return df\n",
    "\n",
    "def transform_data(order_table):\n",
    "    \"\"\"\n",
    "    对订单表进行数据转换,包括缺失值填充，重复值处理，异常数据清理\n",
    "    :param order_table: 订单表\n",
    "    :return: 转换后的订单表\n",
    "    \"\"\"\n",
    "    # 1.判断数据是否为空\n",
    "    if order_table.empty:\n",
    "        logging.warning('输入数据为空，跳过 transform 阶段')\n",
    "        return order_table\n",
    "\n",
    "    # 2.异常数据清理 (识别2016年数据均无效 2018年9-10月数据异常)\n",
    "    order_table = errors_data(order_table)\n",
    "\n",
    "    # 3.缺失值处理\n",
    "    logging.info('[TRANSFORM] 查看缺失值......')\n",
    "    null_count =  order_table.isnull().sum().sum()\n",
    "    logging.info(f'缺失值总数为{null_count}')\n",
    "    if null_count > 0:\n",
    "        # 保存缺失列数据分别情况\n",
    "        loss_data = order_table.isnull().sum()\n",
    "        loss_data = loss_data[loss_data>0]\n",
    "        logging.info(f'缺失列情况: \\n{loss_data}')\n",
    "\n",
    "        for index in loss_data.index:\n",
    "            if index.endswith(('date','at','_timestamp')):\n",
    "                logging.info(f'{index} 为时间列,缺失数:{loss_data[index]}进行时间填充')\n",
    "                order_table[index] = fill_date(order_table[index], index)\n",
    "            else:\n",
    "                logging.info(f'{index} 为普通列，缺失数:{loss_data[index]}，填充为“未知”')\n",
    "                order_table[index] = order_table[index].fillna('未知')\n",
    "        logging.info('缺失值填充完成')\n",
    "    else:\n",
    "        logging.info('无缺失值')\n",
    "\n",
    "    # 4.重复值清理\n",
    "    duplicated_count = order_table.duplicated().sum()\n",
    "    if duplicated_count !=0:\n",
    "        logging.info(f'检测到{duplicated_count}条重复值，执行删除...')\n",
    "        order_table = drop_duplicated(order_table)\n",
    "        logging.info('重复值删除完成')\n",
    "    else:\n",
    "        logging.info('无重复值')\n",
    "    \n",
    "\n",
    "    # 5.特征工程\n",
    "    order_table = feature_engineering(order_table)\n",
    "\n",
    "    return order_table\n",
    "\n",
    "def load_data(order_table):\n",
    "    \"\"\"\n",
    "        将DataFrame保存至指定路径的CSV文件\n",
    "    :param df: 输入DataFrame\n",
    "    :param output_path: 输出文件路径\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    path = save_cleaned / 'order_table_cleaned.csv'\n",
    "    order_table.to_csv(path,index=False)\n",
    "    logging.info(f'[LOAD] 数据已保存至{path}')\n",
    "\n",
    "def fill_date(time_col_data,data_col):\n",
    "    \"\"\"\n",
    "    对时间列缺失值进行填充,填充固定值1970-01-01 00:00:00  仅当占位使用代替NULL无实际意义\n",
    "    :param order_table: 订单表\n",
    "    :param data_col: 时间列\n",
    "    :return: 填充后的订单表\n",
    "    \"\"\"   \n",
    "    try:\n",
    "        filled = time_col_data.fillna('1970-01-01 00:00:00')\n",
    "        logging.info(f'对{data_col}列填充完成')\n",
    "        return filled\n",
    "    except Exception as e:\n",
    "        logging.info(f'对{data_col}列填充失败,失败原因{e}')\n",
    "        \n",
    "def drop_duplicated(order_table):\n",
    "    \"\"\"\n",
    "    对DataFrame删除重复值\n",
    "    :param order_table: 输入DataFrame\n",
    "    :return: 处理后的DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        drop_duplicated_data = order_table.drop_duplicates()\n",
    "        return drop_duplicated_data\n",
    "    except Exception as e:\n",
    "        logging.info(f'删除重复值失败，失败原因{e}')\n",
    "\n",
    "def errors_data(order_table):\n",
    "    order_table['order_purchase_timestamp'] = pd.to_datetime(order_table['order_purchase_timestamp'],errors='coerce')\n",
    "    # 提取年份和月份用于分组\n",
    "    order_table['year'] = order_table['order_purchase_timestamp'].dt.year\n",
    "    order_table['month'] = order_table['order_purchase_timestamp'].dt.month\n",
    "\n",
    "    # 定义要删除的年月组合\n",
    "    periods_to_remove = [\n",
    "        (2016, 9),   # 2016年9月\n",
    "        (2016, 10),  # 2016年10月\n",
    "        (2016, 12),  # 2016年12月\n",
    "        (2018, 9),   # 2018年9月\n",
    "        (2018, 10)   # 2018年10月\n",
    "    ]\n",
    "    # 查看要删除的数据量\n",
    "    for year, month in periods_to_remove:\n",
    "        count = len(order_table[(order_table['year'] == year) & (order_table['month'] == month)])\n",
    "        logging.info(f\"{year}年{month}月的记录数: {count}\")\n",
    "    # 创建删除条件\n",
    "    delete_condition = False\n",
    "    for year, month in periods_to_remove:\n",
    "        delete_condition |= ((order_table['year'] == year) & (order_table['month'] == month))\n",
    "    # 删除指定时间段的数据\n",
    "    order_table = order_table[~delete_condition].copy()\n",
    "    return order_table\n",
    "\n",
    "def feature_engineering(order_table):\n",
    "    # 5.1 year(年) month(月) 根据order_purchase_timestamp 拆分得到\n",
    "        # 其中5.1在errors_data()中做过，此处省略\n",
    "    # 5.2 days(履约日期) 由order_delivered_customer_date - order_purchase_timestamp 得到\n",
    "    order_table['order_delivered_customer_date']= pd.to_datetime(order_table['order_delivered_customer_date'],errors='coerce')\n",
    "    order_table['days'] = (order_table['order_delivered_customer_date'] - order_table['order_purchase_timestamp']).dt.days\n",
    "    order_table.loc[order_table['days']<0,'days'] = -1\n",
    "    return order_table\n",
    "\n",
    "\n",
    "def etl_pipeline(input_path):\n",
    "    order_table_raw = extract_data(input_path)\n",
    "\n",
    "    quality_before = data_quality_report(order_table_raw, '原始数据')\n",
    "\n",
    "    order_table_processed = transform_data(order_table_raw)\n",
    "    quality_after = data_quality_report(order_table_processed, '清洗后数据')\n",
    "\n",
    "    load_data(order_table_processed)\n",
    "\n",
    "    # 生成质量报告汇总表\n",
    "    report_df = pd.DataFrame([quality_before, quality_after])\n",
    "    report_path = save_cleaned / 'data_quality_report.csv'\n",
    "    report_df.to_csv(report_path, index=False, encoding='utf-8-sig')\n",
    "    logging.info(f' 数据质量报告已保存至: {report_path}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    etl_pipeline(data_locate)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
