{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009e601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:42:08,277 [INFO] [EXTRACT] 正在加载数据: [WindowsPath('../raw/order_table_cleaned.csv'), WindowsPath('../raw/olist_order_reviews_dataset.csv')]\n",
      "2025-10-16 15:42:09,031 [INFO] 加载完成，共96095行，8列\n",
      "2025-10-16 15:42:09,107 [INFO] [DATA QUALITY] 原始数据 - 行数: 96095 | 列数: 8 | 缺失率: 18.48% | 重复率: 0.00%\n",
      "2025-10-16 15:42:09,107 [INFO] [TRANSFORM] 查看缺失值......\n",
      "2025-10-16 15:42:09,131 [INFO] 缺失列情况: \n",
      "review_comment_title      84887\n",
      "review_comment_message    57183\n",
      "dtype: int64\n",
      "2025-10-16 15:42:09,131 [INFO] review_comment_title 为普通列，缺失数:84887，填充为“U” 表示未知\n",
      "2025-10-16 15:42:09,137 [INFO] review_comment_message 为普通列，缺失数:57183，填充为“U” 表示未知\n",
      "2025-10-16 15:42:09,147 [INFO] 缺失值填充完成\n",
      "2025-10-16 15:42:09,213 [INFO] 无重复值\n",
      "2025-10-16 15:42:09,292 [INFO] [DATA QUALITY] 清洗后数据 - 行数: 96095 | 列数: 9 | 缺失率: 0.00% | 重复率: 0.00%\n",
      "2025-10-16 15:42:09,534 [INFO] [LOAD] 数据已保存至..\\processed\\order_review_wide.csv\n",
      "2025-10-16 15:42:09,536 [INFO]  数据质量报告已保存至: ..\\processed\\data_quality_report.csv\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import process\n",
    "from nt import error\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"etl.log\", encoding='utf-8'), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "order_data_locate = Path('../raw/order_table_cleaned.csv')\n",
    "review_data_locate = Path('../raw/olist_order_reviews_dataset.csv')\n",
    "save_cleaned = Path('../processed')\n",
    "save_cleaned.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def data_quality_report(df,stage_name):\n",
    "    \"\"\"\n",
    "    输出数据质量报告（重复率、缺失率等）\n",
    "    :param df: DataFrame\n",
    "    :param stage_name: 阶段名称，如 '原始数据' 或 '清洗后的数据'\n",
    "    :return: dict (用于后续保存或对比)\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    total_cols = df.shape[1]\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    duplicated_count = df.duplicated().sum()\n",
    "\n",
    "    # 计算比例\n",
    "    missing_rate = (missing_count / (total_rows * total_cols) * 100) if total_rows > 0 else 0\n",
    "    duplicated_rate = (duplicated_count / total_rows * 100) if total_rows > 0 else 0\n",
    "\n",
    "    # 打印日志\n",
    "    logging.info(f'[DATA QUALITY] {stage_name} - 行数: {total_rows} | 列数: {total_cols} | 缺失率: {missing_rate:.2f}% | 重复率: {duplicated_rate:.2f}%')\n",
    "\n",
    "    # 返回结构化结果，方便后面汇总\n",
    "    return {\n",
    "        'stage': stage_name,\n",
    "        'rows': total_rows,\n",
    "        'cols': total_cols,\n",
    "        'missing_count': missing_count,\n",
    "        'missing_rate': round(missing_rate, 2),\n",
    "        'duplicated_count': duplicated_count,\n",
    "        'duplicated_rate': round(duplicated_rate, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_data(path_list):\n",
    "    \"\"\"\n",
    "        从指定路径加载CSV文件\n",
    "    :param path_list: CSV文件路径列表\n",
    "    :return: 加载后的DataFrame (订单和评论宽表)\n",
    "    \"\"\"\n",
    "    logging.info(f'[EXTRACT] 正在加载数据: {path_list}')\n",
    "    try:\n",
    "        order_table = pd.read_csv(path_list[0])\n",
    "        review_table = pd.read_csv(path_list[1])\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f'文件未找到: {path_list}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cond = order_table['order_status'] == 'delivered'\n",
    "    \n",
    "    order_table = order_table.loc[cond,['order_id','days']]\n",
    "    order_review_wide = review_table.merge(order_table,how='inner',on='order_id')\n",
    "    row,col = order_review_wide.shape\n",
    "    logging.info(f'加载完成，共{row}行，{col}列')\n",
    "    return order_review_wide\n",
    "\n",
    "def fill_date(time_col_data,data_col):\n",
    "    \"\"\"\n",
    "    对时间列缺失值进行填充,填充固定值1970-01-01 00:00:00  仅当占位使用代替NULL无实际意义\n",
    "    :param time_col_data 时间列数据\n",
    "    :param data_col: 时间列名\n",
    "    :return: 填充后的无缺失值的时间列数据\n",
    "    \"\"\"   \n",
    "    try:\n",
    "        filled = time_col_data.fillna('1970-01-01 00:00:00')\n",
    "        logging.info(f'对{data_col}列填充完成')\n",
    "        return filled\n",
    "    except Exception as e:\n",
    "        logging.info(f'对{data_col}列填充失败,失败原因{e}')\n",
    "       \n",
    "def drop_duplicated(order_review_wide):\n",
    "    \"\"\"\n",
    "    对DataFrame删除重复值\n",
    "    :param order_table: 输入DataFrame\n",
    "    :return: 处理后的DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        drop_duplicated_data = order_review_wide.drop_duplicates()\n",
    "        return drop_duplicated_data\n",
    "    except Exception as e:\n",
    "        logging.info(f'删除重复值失败，失败原因{e}')\n",
    "\n",
    "def feature_engineering(order_review_wide):\n",
    "    # 设定区间范围和标签\n",
    "    bins = [0, 7, 14, 30, 180]\n",
    "    labels = ['0-7天', '8-14天', '15-30天', '>30天']\n",
    "\n",
    "    order_review_wide['bucket'] = pd.cut(\n",
    "        order_review_wide['days'],\n",
    "        bins=bins,\n",
    "        labels=labels,\n",
    "        right=True,\n",
    "        include_lowest=True\n",
    "    )\n",
    "    return order_review_wide\n",
    "\n",
    "def transform_data(order_review_wide):\n",
    "    \"\"\"\n",
    "    对订单表进行数据转换,包括缺失值填充，重复值处理，异常数据清理\n",
    "    :param order_review_wide: 评分+订单宽表\n",
    "    :return: 转换后的订单表\n",
    "    \"\"\"\n",
    "    # 1.判断数据是否为空\n",
    "    if order_review_wide.empty:\n",
    "        logging.warning('输入数据为空，跳过 transform 阶段')\n",
    "        return order_review_wide\n",
    "\n",
    "    # 2.缺失值处理\n",
    "    logging.info('[TRANSFORM] 查看缺失值......')\n",
    "    null_count =  order_review_wide.isnull().sum().sum()\n",
    "    if null_count > 0:\n",
    "        # 保存缺失列数据分别情况\n",
    "        loss_data = order_review_wide.isnull().sum()\n",
    "        loss_data = loss_data[loss_data>0]\n",
    "        logging.info(f'缺失列情况: \\n{loss_data}')\n",
    "\n",
    "        for index in loss_data.index:\n",
    "            if index.endswith(('_date','_timestamp')):\n",
    "                logging.info(f'{index} 为时间列,缺失数:{loss_data[index]}进行时间填充')\n",
    "                order_review_wide[index] = fill_date(order_review_wide[index], index)\n",
    "            else:\n",
    "                logging.info(f'{index} 为普通列，缺失数:{loss_data[index]}，填充为“U” 表示未知')\n",
    "                order_review_wide[index] = order_review_wide[index].fillna('U')\n",
    "        logging.info('缺失值填充完成')\n",
    "    else:\n",
    "        logging.info('无缺失值')\n",
    "\n",
    "    # 3.重复值清理\n",
    "    duplicated_count = order_review_wide.duplicated().sum()\n",
    "    if duplicated_count !=0:\n",
    "        logging.info(f'检测到{duplicated_count}条重复值，执行删除...')\n",
    "        order_review_wide = drop_duplicated(order_review_wide)\n",
    "        logging.info('重复值删除完成')\n",
    "    else:\n",
    "        logging.info('无重复值')\n",
    "    \n",
    "\n",
    "    # 4.特征工程\n",
    "    order_review_wide = feature_engineering(order_review_wide)\n",
    "\n",
    "    return order_review_wide\n",
    "\n",
    "def load_data(order_review_wide):\n",
    "    \"\"\"\n",
    "        将DataFrame保存至指定路径的CSV文件\n",
    "    :param order_review_wide: 输入DataFrame\n",
    "    :param output_path: 输出文件路径\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    path = save_cleaned / 'order_review_wide.csv'\n",
    "    order_review_wide.to_csv(path,index=False)\n",
    "    logging.info(f'[LOAD] 数据已保存至{path}')\n",
    "\n",
    "def etl_pipeline(input_path):\n",
    "    order_review_wide_raw = extract_data(input_path)\n",
    "\n",
    "    quality_before = data_quality_report(order_review_wide_raw, '原始数据')\n",
    "\n",
    "    order_review_wide_processed = transform_data(order_review_wide_raw)\n",
    "    quality_after = data_quality_report(order_review_wide_processed, '清洗后数据')\n",
    "\n",
    "    load_data(order_review_wide_processed)\n",
    "\n",
    "    # 生成质量报告汇总表\n",
    "    report_df = pd.DataFrame([quality_before, quality_after])\n",
    "    report_path = save_cleaned / 'data_quality_report.csv'\n",
    "    report_df.to_csv(report_path, index=False, encoding='utf-8-sig')\n",
    "    logging.info(f' 数据质量报告已保存至: {report_path}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path_list = [order_data_locate,review_data_locate]\n",
    "    etl_pipeline(path_list)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
